{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29843459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from olympus.datasets import Dataset\n",
    "from olympus.objects import ParameterVector\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "tkwargs = {'device': 'cpu', 'dtype': torch.double}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48bb099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_a = Dataset(kind='buchwald_a')\n",
    "dset_b = Dataset(kind='buchwald_b')\n",
    "dset_c = Dataset(kind='buchwald_c')\n",
    "dset_d = Dataset(kind='buchwald_d')\n",
    "dset_e = Dataset(kind='buchwald_e')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9904ad77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aryl_halide', 'additive', 'base', 'ligand']\n"
     ]
    }
   ],
   "source": [
    "param_names = list(dset_a.data.columns)[:-1]\n",
    "print(param_names)\n",
    "\n",
    "params_a = dset_a.data[param_names].values\n",
    "params_pvec_a = []\n",
    "for param in params_a:\n",
    "    params_pvec_a.append(ParameterVector().from_dict(dict(zip(param_names, param)), param_space=dset_a.param_space))\n",
    "\n",
    "    \n",
    "params_b = dset_b.data[param_names].values\n",
    "params_pvec_b = []\n",
    "for param in params_b:\n",
    "    params_pvec_b.append(ParameterVector().from_dict(dict(zip(param_names, param)), param_space=dset_a.param_space))\n",
    "    \n",
    "    \n",
    "params_c = dset_c.data[param_names].values\n",
    "params_pvec_c = []\n",
    "for param in params_c:\n",
    "    params_pvec_c.append(ParameterVector().from_dict(dict(zip(param_names, param)), param_space=dset_a.param_space))\n",
    "    \n",
    "    \n",
    "params_d = dset_d.data[param_names].values\n",
    "params_pvec_d = []\n",
    "for param in params_d:\n",
    "    params_pvec_d.append(ParameterVector().from_dict(dict(zip(param_names, param)), param_space=dset_a.param_space))\n",
    "\n",
    "    \n",
    "params_e = dset_e.data[param_names].values\n",
    "params_pvec_e = []\n",
    "for param in params_e:\n",
    "    params_pvec_e.append(ParameterVector().from_dict(dict(zip(param_names, param)), param_space=dset_a.param_space))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ecf749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_a = []\n",
    "for param in params_pvec_a:\n",
    "    obj_a.append(dset_a.run(param, noiseless=True)[0][0])\n",
    "    \n",
    "obj_b = []\n",
    "for param in params_pvec_b:\n",
    "    obj_b.append(dset_b.run(param, noiseless=True)[0][0])\n",
    "    \n",
    "obj_c = []\n",
    "for param in params_pvec_c:\n",
    "    obj_c.append(dset_c.run(param, noiseless=True)[0][0])\n",
    "    \n",
    "obj_d = []\n",
    "for param in params_pvec_d:\n",
    "    obj_d.append(dset_d.run(param, noiseless=True)[0][0])\n",
    "    \n",
    "obj_e = []\n",
    "for param in params_pvec_e:\n",
    "    obj_e.append(dset_e.run(param, noiseless=True)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ced7bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597\n",
      "55.56585889\n",
      "ParamVector(aryl_halide = FC(F)(F)c1ccc(I)cc1, additive = Cc1ccon1, base = CN1CCCN2CCCN=C12, ligand = CC(C1=C(C2=C(OC)C=CC(OC)=C2P(C34CC5CC(C4)CC(C5)C3)C67CC8CC(C7)CC(C8)C6)C(C(C)C)=CC(C(C)C)=C1)C)\n",
      "\n",
      "\n",
      "740\n",
      "68.2481271\n",
      "ParamVector(aryl_halide = COc1ccc(I)cc1, additive = C(N(Cc1ccccc1)c2ccon2)c3ccccc3, base = CN1CCCN2CCCN=C12, ligand = CC(C1=C(C2=C(OC)C=CC(OC)=C2P(C34CC5CC(C4)CC(C5)C3)C67CC8CC(C7)CC(C8)C6)C(C(C)C)=CC(C(C)C)=C1)C)\n",
      "\n",
      "\n",
      "276\n",
      "86.59757822\n",
      "ParamVector(aryl_halide = CCc1ccc(Br)cc1, additive = CCOC(=O)c1onc(C)c1, base = CN1CCCN2CCCN=C12, ligand = CC(C)C(C=C(C(C)C)C=C1C(C)C)=C1C2=CC=CC=C2P(C(C)(C)C)C(C)(C)C)\n",
      "\n",
      "\n",
      "575\n",
      "99.99999\n",
      "ParamVector(aryl_halide = Ic1ccccn1, additive = o1cc(cn1)c2ccccc2, base = CN1CCCN2CCCN=C12, ligand = CC(C)C(C=C(C(C)C)C=C1C(C)C)=C1C2=CC=CC=C2P(C(C)(C)C)C(C)(C)C)\n",
      "\n",
      "\n",
      "275\n",
      "98.73132029\n",
      "ParamVector(aryl_halide = Brc1cccnc1, additive = o1nccc1c2ccccc2, base = CN1CCCN2CCCN=C12, ligand = CC(C)C(C=C(C(C)C)C=C1C(C)C)=C1C2=CC=CC=C2P(C(C)(C)C)C(C)(C)C)\n"
     ]
    }
   ],
   "source": [
    "best_idx_a = np.argmax(obj_a)\n",
    "best_obj_a = obj_a[best_idx_a]\n",
    "best_param_a = params_pvec_a[best_idx_a]\n",
    "print(best_idx_a)\n",
    "print(best_obj_a)\n",
    "print(best_param_a)\n",
    "\n",
    "\n",
    "best_idx_b = np.argmax(obj_b)\n",
    "best_obj_b = obj_b[best_idx_b]\n",
    "best_param_b = params_pvec_b[best_idx_b]\n",
    "print('\\n')\n",
    "print(best_idx_b)\n",
    "print(best_obj_b)\n",
    "print(best_param_b)\n",
    "\n",
    "best_idx_c = np.argmax(obj_c)\n",
    "best_obj_c = obj_c[best_idx_c]\n",
    "best_param_c = params_pvec_c[best_idx_c]\n",
    "print('\\n')\n",
    "print(best_idx_c)\n",
    "print(best_obj_c)\n",
    "print(best_param_c)\n",
    "\n",
    "best_idx_d = np.argmax(obj_d)\n",
    "best_obj_d = obj_d[best_idx_d]\n",
    "best_param_d = params_pvec_d[best_idx_d]\n",
    "print('\\n')\n",
    "print(best_idx_d)\n",
    "print(best_obj_d)\n",
    "print(best_param_d)\n",
    "\n",
    "best_idx_e = np.argmax(obj_e)\n",
    "best_obj_e = obj_e[best_idx_e]\n",
    "best_param_e = params_pvec_e[best_idx_e]\n",
    "print('\\n')\n",
    "print(best_idx_e)\n",
    "print(best_obj_e)\n",
    "print(best_param_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6675c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vals = {\n",
    "    'a': {'best_idx': best_idx_a, 'best_obj': best_obj_a, 'best_params': best_param_a},\n",
    "    'b': {'best_idx': best_idx_b, 'best_obj': best_obj_b, 'best_params': best_param_b},\n",
    "    'c': {'best_idx': best_idx_c, 'best_obj': best_obj_c, 'best_params': best_param_c},\n",
    "    'd': {'best_idx': best_idx_d, 'best_obj': best_obj_d, 'best_params': best_param_d},\n",
    "    'e': {'best_idx': best_idx_e, 'best_obj': best_obj_e, 'best_params': best_param_e},\n",
    "}\n",
    "pickle.dump(best_vals, open('best_vals_buchwald_ae.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca2d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fbbaf38",
   "metadata": {},
   "source": [
    "### Concatenate together source tasks for buchwald_{a-e} problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a15c878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_source = pickle.load(open('all_15_source_tasks/tasks_aryl_one_hot.pkl', 'rb'))\n",
    "\n",
    "# 5 datasets, sets of 3 need to be concatenated together\n",
    "\n",
    "idx_map = {\n",
    "    'a': [0, 1, 2],\n",
    "    'b': [3, 4, 5],\n",
    "    'c': [6, 7, 8],\n",
    "    'd': [9, 10, 11],\n",
    "    'e': [12, 13, 14],\n",
    "}\n",
    "source_sets = list(idx_map.keys())\n",
    "source_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2f18e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(828, 30) (828, 1)\n",
      "(825, 30) (825, 1)\n",
      "(826, 30) (826, 1)\n",
      "(827, 30) (827, 1)\n"
     ]
    }
   ],
   "source": [
    "target_set = 'a'\n",
    "source_tasks_a = []\n",
    "for source_set in source_sets:\n",
    "    if source_set != target_set:\n",
    "        source_params = np.concatenate([all_source[idx]['params'] for idx in idx_map[source_set]])\n",
    "        source_values = np.concatenate([all_source[idx]['values'] for idx in idx_map[source_set]])\n",
    "        print(source_params.shape, source_values.shape)\n",
    "        source_tasks_a.append({'params': source_params, 'values': source_values})\n",
    "        \n",
    "pickle.dump(source_tasks_a, open('ae_source_tasks/source_tasks_a.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "386c26c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(826, 30) (826, 1)\n",
      "(825, 30) (825, 1)\n",
      "(826, 30) (826, 1)\n",
      "(827, 30) (827, 1)\n"
     ]
    }
   ],
   "source": [
    "target_set = 'b'\n",
    "source_tasks_b = []\n",
    "for source_set in source_sets:\n",
    "    if source_set != target_set:\n",
    "        source_params = np.concatenate([all_source[idx]['params'] for idx in idx_map[source_set]])\n",
    "        source_values = np.concatenate([all_source[idx]['values'] for idx in idx_map[source_set]])\n",
    "        print(source_params.shape, source_values.shape)\n",
    "        source_tasks_b.append({'params': source_params, 'values': source_values})\n",
    "        \n",
    "pickle.dump(source_tasks_b, open('ae_source_tasks/source_tasks_b.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a44dbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(826, 30) (826, 1)\n",
      "(828, 30) (828, 1)\n",
      "(826, 30) (826, 1)\n",
      "(827, 30) (827, 1)\n"
     ]
    }
   ],
   "source": [
    "target_set = 'c'\n",
    "source_tasks_c = []\n",
    "for source_set in source_sets:\n",
    "    if source_set != target_set:\n",
    "        source_params = np.concatenate([all_source[idx]['params'] for idx in idx_map[source_set]])\n",
    "        source_values = np.concatenate([all_source[idx]['values'] for idx in idx_map[source_set]])\n",
    "        print(source_params.shape, source_values.shape)\n",
    "        source_tasks_c.append({'params': source_params, 'values': source_values})\n",
    "        \n",
    "pickle.dump(source_tasks_c, open('ae_source_tasks/source_tasks_c.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd7f7885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(826, 30) (826, 1)\n",
      "(828, 30) (828, 1)\n",
      "(825, 30) (825, 1)\n",
      "(827, 30) (827, 1)\n"
     ]
    }
   ],
   "source": [
    "target_set = 'd'\n",
    "source_tasks_d = []\n",
    "for source_set in source_sets:\n",
    "    if source_set != target_set:\n",
    "        source_params = np.concatenate([all_source[idx]['params'] for idx in idx_map[source_set]])\n",
    "        source_values = np.concatenate([all_source[idx]['values'] for idx in idx_map[source_set]])\n",
    "        print(source_params.shape, source_values.shape)\n",
    "        source_tasks_d.append({'params': source_params, 'values': source_values})\n",
    "        \n",
    "pickle.dump(source_tasks_d, open('ae_source_tasks/source_tasks_d.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abfccd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(826, 30) (826, 1)\n",
      "(828, 30) (828, 1)\n",
      "(825, 30) (825, 1)\n",
      "(826, 30) (826, 1)\n"
     ]
    }
   ],
   "source": [
    "target_set = 'e'\n",
    "source_tasks_e = []\n",
    "for source_set in source_sets:\n",
    "    if source_set != target_set:\n",
    "        source_params = np.concatenate([all_source[idx]['params'] for idx in idx_map[source_set]])\n",
    "        source_values = np.concatenate([all_source[idx]['values'] for idx in idx_map[source_set]])\n",
    "        print(source_params.shape, source_values.shape)\n",
    "        source_tasks_e.append({'params': source_params, 'values': source_values})\n",
    "        \n",
    "pickle.dump(source_tasks_e, open('ae_source_tasks/source_tasks_e.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b3cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d42c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a353b61",
   "metadata": {},
   "source": [
    "## Re-do the OHE of parameter for buchwald_{a-e} problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "010ca3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = Dataset(kind='buchwald_a')\n",
    "dataset_b = Dataset(kind='buchwald_b')\n",
    "dataset_c = Dataset(kind='buchwald_c')\n",
    "dataset_d = Dataset(kind='buchwald_d')\n",
    "dataset_e = Dataset(kind='buchwald_e')\n",
    "# for dataset in [dataset_a,dataset_b,dataset_c,dataset_d,dataset_e]:\n",
    "#     print(dataset)\n",
    "#     print(dataset.data.shape)\n",
    "#     for param in dataset.param_space:\n",
    "#         print(param.name,len(dataset.data[param.name].unique()))\n",
    "\n",
    "all_aryl_opts = []\n",
    "all_additive_opts = []\n",
    "all_base_opts = []\n",
    "all_ligand_opts = []\n",
    "\n",
    "for dataset in [dataset_a, dataset_b, dataset_c, dataset_d, dataset_e]:\n",
    "    \n",
    "    all_aryl_opts.extend(dataset.param_space[0].options)\n",
    "    all_additive_opts.extend(dataset.param_space[1].options)\n",
    "    all_base_opts.extend(dataset.param_space[2].options)\n",
    "    all_ligand_opts.extend(dataset.param_space[3].options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98caa88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FC(F)(F)c1ccc(Cl)cc1', 'FC(F)(F)c1ccc(Br)cc1', 'FC(F)(F)c1ccc(I)cc1', 'COc1ccc(Cl)cc1', 'COc1ccc(Br)cc1', 'COc1ccc(I)cc1', 'CCc1ccc(Cl)cc1', 'CCc1ccc(Br)cc1', 'CCc1ccc(I)cc1', 'Clc1ccccn1', 'Brc1ccccn1', 'Ic1ccccn1', 'Clc1cccnc1', 'Brc1cccnc1', 'Ic1cccnc1']\n",
      "15\n",
      "22\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(all_aryl_opts)\n",
    "\n",
    "print( len(list(set(all_aryl_opts))) )\n",
    "print( len(list(set(all_additive_opts))) )\n",
    "print( len(list(set(all_base_opts))) )\n",
    "print( len(list(set(all_ligand_opts))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af7fc6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_param(param, param_space):\n",
    "    ohe_comps = []\n",
    "    for param_ix, p in enumerate(param):\n",
    "        opt_ix = param_space[param_ix].options.index(p)\n",
    "        ohe = np.zeros(len(param_space[param_ix].options))\n",
    "        ohe[opt_ix] = 1.\n",
    "        ohe_comps.append(ohe)\n",
    "        \n",
    "    return np.concatenate(ohe_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38fb3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = dataset_a.data\n",
    "source_params = []\n",
    "source_values = []\n",
    "for ix, row in df_a.iterrows():\n",
    "    row = row.to_dict()\n",
    "    params = list(row.values())[:-1]\n",
    "    yield_ = row['yield']\n",
    "    ohe = encode_param(params, dataset_a.param_space)\n",
    "    source_params.append(ohe)\n",
    "    source_values.append(yield_)\n",
    "    \n",
    "source_params = torch.tensor(np.stack(source_params), **tkwargs)\n",
    "source_values = torch.tensor(np.stack(source_values).reshape(-1, 1), **tkwargs)\n",
    "\n",
    "task_a = {'params': source_params, 'values': source_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e77fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = dataset_b.data\n",
    "source_params = []\n",
    "source_values = []\n",
    "for ix, row in df_b.iterrows():\n",
    "    row = row.to_dict()\n",
    "    params = list(row.values())[:-1]\n",
    "    yield_ = row['yield']\n",
    "    ohe = encode_param(params, dataset_b.param_space)\n",
    "    source_params.append(ohe)\n",
    "    source_values.append(yield_)\n",
    "    \n",
    "source_params = torch.tensor(np.stack(source_params), **tkwargs)\n",
    "source_values = torch.tensor(np.stack(source_values).reshape(-1, 1), **tkwargs)\n",
    "\n",
    "task_b = {'params': source_params, 'values': source_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f9e93404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = dataset_c.data\n",
    "source_params = []\n",
    "source_values = []\n",
    "for ix, row in df_c.iterrows():\n",
    "    row = row.to_dict()\n",
    "    params = list(row.values())[:-1]\n",
    "    yield_ = row['yield']\n",
    "    ohe = encode_param(params, dataset_c.param_space)\n",
    "    source_params.append(ohe)\n",
    "    source_values.append(yield_)\n",
    "    \n",
    "source_params = torch.tensor(np.stack(source_params), **tkwargs)\n",
    "source_values = torch.tensor(np.stack(source_values).reshape(-1, 1), **tkwargs)\n",
    "\n",
    "task_c = {'params': source_params, 'values': source_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92200f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = dataset_d.data\n",
    "source_params = []\n",
    "source_values = []\n",
    "for ix, row in df_d.iterrows():\n",
    "    row = row.to_dict()\n",
    "    params = list(row.values())[:-1]\n",
    "    yield_ = row['yield']\n",
    "    ohe = encode_param(params, dataset_d.param_space)\n",
    "    source_params.append(ohe)\n",
    "    source_values.append(yield_)\n",
    "    \n",
    "source_params = torch.tensor(np.stack(source_params), **tkwargs)\n",
    "source_values = torch.tensor(np.stack(source_values).reshape(-1, 1), **tkwargs)\n",
    "\n",
    "task_d = {'params': source_params, 'values': source_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "76b8482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = dataset_e.data\n",
    "source_params = []\n",
    "source_values = []\n",
    "for ix, row in df_e.iterrows():\n",
    "    row = row.to_dict()\n",
    "    params = list(row.values())[:-1]\n",
    "    yield_ = row['yield']\n",
    "    ohe = encode_param(params, dataset_e.param_space)\n",
    "    source_params.append(ohe)\n",
    "    source_values.append(yield_)\n",
    "    \n",
    "source_params = torch.tensor(np.stack(source_params), **tkwargs)\n",
    "source_values = torch.tensor(np.stack(source_values).reshape(-1, 1), **tkwargs)\n",
    "\n",
    "task_e = {'params': source_params, 'values': source_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4eb3d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_target_a = [task_b, task_c, task_d, task_e]\n",
    "tasks_target_b = [task_a, task_c, task_d, task_e]\n",
    "tasks_target_c = [task_a, task_b, task_d, task_e]\n",
    "tasks_target_d = [task_a, task_b, task_c, task_e]\n",
    "tasks_target_e = [task_a, task_b, task_c, task_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0f2bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tasks_target_a, open('ae_source_tasks/source_tasks_a.pkl', 'wb'))\n",
    "pickle.dump(tasks_target_b, open('ae_source_tasks/source_tasks_b.pkl', 'wb'))\n",
    "pickle.dump(tasks_target_c, open('ae_source_tasks/source_tasks_c.pkl', 'wb'))\n",
    "pickle.dump(tasks_target_d, open('ae_source_tasks/source_tasks_d.pkl', 'wb'))\n",
    "pickle.dump(tasks_target_e, open('ae_source_tasks/source_tasks_e.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e6926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas_aag",
   "language": "python",
   "name": "atlas_aag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
